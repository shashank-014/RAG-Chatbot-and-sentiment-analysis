#“Domain-Specific RAG Chatbot for Technical Knowledge Retrieval”

A Retrieval-Augmented Generation chatbot that answers questions from technical documents using embeddings, a vector database (FAISS), and an LLM.

#step 1: installing dependencies

!pip uninstall -y langchain langchain-core langchain-community langchain-openai langchain-huggingface torch torchvision transformers accelerate sentencepiece tokenizers

# Install compatible core libraries first (PyTorch CPU versions)
!pip install torch==2.10.0 torchvision==0.25.0 --extra-index-url https://download.pytorch.org/whl/cpu

# Install transformers and its related libraries. transformers 4.35.2 is a good stable choice.
!pip install transformers==4.35.2 accelerate sentencepiece tokenizers

# Install langchain and its components (which should now find compatible transformers)
!pip install -U \
  langchain \
  langchain-core \
  langchain-community \
  langchain-openai \
  langchain-huggingface \
  faiss-cpu \
  sentence-transformers \
  pypdf \
  tiktoken

!pip install -U transformers accelerate torch

!pip uninstall -y \
  langchain \
  langchain-core \
  langchain-community \
  langchain-openai \
  langchain-huggingface \
  langchain-google-genai \
  torch \
  torchvision \
  transformers \
  accelerate \
  sentencepiece \
  tokenizers \
  faiss-cpu \
  sentence-transformers \
  pypdf \
  tiktoken \
  google-generativeai

# Core ML stack (CPU only, stable)
!pip install torch==2.1.2 --index-url https://download.pytorch.org/whl/cpu

# Transformers stack (stable, no pipeline bugs)
!pip install transformers==4.36.2 accelerate sentencepiece tokenizers

# LangChain (MODERN modular stack)
!pip install \
  langchain==0.1.20 \
  langchain-core==0.1.52 \
  langchain-community==0.0.38 \
  langchain-huggingface==0.0.3 \
  faiss-cpu \
  pypdf


!pip install torchvision --index-url https://download.pytorch.org/whl/cpu

#step 2: importing libraries

# PDF loader
from langchain_community.document_loaders import PyPDFLoader

# Text splitter (IMPORTANT)
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Embeddings
from langchain_huggingface import HuggingFaceEmbeddings

# Vector store
from langchain_community.vectorstores import FAISS

# Chains
from langchain.chains import RetrievalQA

print("✅ All imports successful")

#step 3: uploading of documents

from google.colab import files
files.upload()

#step 4: loading the documents

from langchain_community.document_loaders import PyPDFLoader

pdf_paths = [
    "/content/micron-intro-to-fabrication-presentation.pdf",
    "/content/LaboratoryManualForSemiconductorDevices.pdf",
    "/content/Yield_Analysis_in_Semiconductor_Manufacturing_Tech.pdf"
]

documents = []

for path in pdf_paths:
    loader = PyPDFLoader(path)
    documents.extend(loader.load())

print(f"Total pages loaded: {len(documents)}")

documents[0].page_content[:300]

#step 5: chunk the documents

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ".", " ", ""]
)

chunks = text_splitter.split_documents(documents)

print(f"Total chunks created: {len(chunks)}")

chunks[0].page_content

#step 6: creating embeddings

from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

test_vec = embeddings.embed_query("What is wafer yield?")
print(len(test_vec))

#step 7: storing in vector database--FAISS

from langchain_community.vectorstores import FAISS

vectorstore = FAISS.from_documents(
    documents=chunks,
    embedding=embeddings
)

print("FAISS index created successfully ✅")

#checking vector count
vectorstore.index.ntotal

#test semantic retrieval
query = "What is wafer yield in semiconductor manufacturing?"
docs = vectorstore.similarity_search(query, k=3)

for i, d in enumerate(docs, 1):
    print(f"\n--- Result {i} ---")
    print(d.page_content[:300])

vectorstore.save_local("faiss_semiconductor_index")

vectorstore = FAISS.load_local(
    "faiss_semiconductor_index",
    embeddings,
    allow_dangerous_deserialization=True
)

"I persisted the FAISS index so embeddings don't need to be recomputed."

#step 8: connecting LLM(Hugging Face LLM-free-no api key)

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
from langchain.llms.base import LLM
from typing import Optional, List

model_name = "google/flan-t5-large"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

model.eval()

class FlanT5LLM(LLM):
    max_new_tokens: int = 256

    @property
    def _llm_type(self) -> str:
        return "flan-t5"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
      instruction = (
          "You are a semiconductor manufacturing expert.\n"
          "Answer the question in one complete explanatory sentence.\n"
          "Do NOT give a short phrase.\n\n"
          f"Question: {prompt}\n"
          "Answer:"
            )

      inputs = tokenizer(
         instruction,
         return_tensors="pt",
         truncation=True
         )

      with torch.no_grad():
          output_ids = model.generate(
          **inputs,
          max_new_tokens=300,
          min_new_tokens=120,   # forces real explanation
          do_sample=False
           )


      return tokenizer.decode(
          output_ids[0],
          skip_special_tokens=True
          )


llm = FlanT5LLM()
print("FLAN-T5 LLM ready (NO pipeline, NO errors)")

llm.invoke("What is wafer yield?")

llm.invoke("Explain in one complete sentence: What is wafer yield in semiconductor manufacturing?")

#step 9

from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are a senior semiconductor process engineer.

Using ONLY the context below:
- Explain the concept clearly
- Then explain HOW it is improved in practice
- Use 3–4 complete sentences
- Mention process control, defect reduction, and yield analysis if relevant

Context:
{context}

Question:
{question}

Answer:
"""
)


qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True
)

print("RAG pipeline ready")


qa_chain.invoke({"query": "How is wafer yield improved in semiconductor manufacturing?"})

#step 10: asking questions

qa_chain.invoke({
    "query": "How is wafer yield improved in semiconductor manufacturing?"
})["result"]


query = "What is wafer yield and how is it improved?"
result = qa_chain.invoke({"query": query})

print("ANSWER:\n", result["result"])

for doc in result["source_documents"]:
    print("\nSOURCE:", doc.metadata)

query = "What are common troubleshooting steps mentioned in the document?"
result = qa_chain(query)

print(result["result"])

from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are a semiconductor manufacturing engineer.

Below is an example of how to answer:

Example:
Question: How is defect density reduced?
Answer:
Step 1: Monitor lithography and etching steps using inspection tools.
Step 2: Apply statistical process control to critical parameters.
Step 3: Analyze defect maps and apply corrective actions.

Now answer the following question in the SAME FORMAT.
Use at least 3 steps.
Each step must be a full sentence describing a real manufacturing action.

Context:
{context}

Question:
{question}

Answer:
"""
)


qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 6}),
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True
)


qa_chain.invoke({
    "query": "How is wafer yield improved in semiconductor manufacturing?"
})["result"]


#step 11: simple chat loop

while True:
    q = input("Ask a question (or type exit): ")
    if q.lower() == "exit":
        break
    response = qa_chain(q)
    print("\nAnswer:", response["result"], "\n")

#for working output, please go to colab link : https://colab.research.google.com/drive/1LcCNRzi3foX24H3hZu4JSrowoi90IurX#scrollTo=2vccs33D-ozn 
